{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main objective - demystified \n",
    "\n",
    "We said that we wanted **classify** small subgraphs into one of the subnetworks for each dataset. For example, given a small *directed temporal subgraph* from the email network, we want to determine to which department it belongs to.  \n",
    "\n",
    "We can achieve this in 3 steps:\n",
    "1. Group together the timestamped edges so that they form small subgraphs [sampling]\n",
    "2. Transform these small graphs into feature vectors [embedding]\n",
    "3. Train various machine learning models using the vectors generated in the previous step and classify unknown samples [modeling]\n",
    "\n",
    "\n",
    "## Sampling\n",
    "As for any machine learning problem, we first need to define what the samples are. In this case, we only have the network as a whole for each of the classes. So we need to break it up.  \n",
    "\n",
    "A fairly reasonable way to partition the network is to break it into **time-windows** with variable sizes that span fixed amount of time. This way, not only do we preserve the temporal motif formations, but we also get a sense of how they evolved throughout the years.  \n",
    "I know this is a mouthful, but it's a pretty simple process. First, we sort the network (which is a list of timestamped edges) by the timestamp value, then we form time-windows that span 24-hour period. From here, we build 2 datasets: \n",
    "* each sample is a single window\n",
    "* each sample is a concatenation of itself and its two previous windows  \n",
    "\n",
    "The first alternative can hurt the models in a sense that it loses a sense of context temporality as a result of the discretization performed on the graph - any information in the neighboring time-frames is lost.  \n",
    "However, the second alternative will produce samples with more context - what was happening in the previous two time-frames. Not only that, but we preserve the overall number of samples in the dataset - if we were to only concatenate each triplet of time-windows without overlapping, we would end up with samples that have as much temporal context as the second alternative, but the resulting dataset would have 3 times less samples than the one produced in the second alternative, which will definitely hurt the models since we're already dealing with limited number of samples.\n",
    "\n",
    "## Embedding\n",
    "Now that we have defined what our samples are, we need to embed them - transform them into vectors. How exactly are we going to achieve this? **Motifs** come to the rescue. Each subgraph will be represented in 3 ways:\n",
    "1. With its **temporal motif** distribution - a vector of 36 values, each one representing the count of the temporal motifs described in \"Motifs in Temporal Networks\" by Jure Leskovec  \n",
    "2. With its **static motif** distribution - a vector of 13 values, each one representing the count of the static motifs described in \"Efficient Detection of Network Motifs\" by \n",
    "Sebastian Wernicke\n",
    "3. A combination of the previous two  \n",
    "\n",
    "## Modeling\n",
    "After embedding the subgraphs, we construct the datasets and split them in train/test sets. In this notebook we only go over the data preparation step. The model definition step will be discussed in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_email_dept_1 = pd.read_csv('data/formatted/email/dept_1.csv', delim_whitespace=True)\n",
    "df_email_dept_2 = pd.read_csv('data/formatted/email/dept_2.csv', delim_whitespace=True)\n",
    "df_email_dept_3 = pd.read_csv('data/formatted/email/dept_3.csv', delim_whitespace=True)\n",
    "df_email_dept_4 = pd.read_csv('data/formatted/email/dept_4.csv', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sx_superuser = pd.read_csv('data/formatted/sx/superuser.csv', delim_whitespace=True)\n",
    "df_sx_askubuntu = pd.read_csv('data/formatted/sx/askubuntu.csv', delim_whitespace=True)\n",
    "df_sx_mathoverflow = pd.read_csv('data/formatted/sx/mathoverflow.csv', delim_whitespace=True)\n",
    "df_sx_stackoverflow = pd.read_csv('data/formatted/sx/stackoverflow.csv', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [(df_email_dept_1, 'dept_1'), (df_email_dept_2, 'dept_2'), (df_email_dept_3, 'dept_3'), (df_email_dept_4, 'dept_4'), \n",
    "            (df_sx_superuser, 'superuser'), (df_sx_askubuntu, 'askubuntu'), (df_sx_mathoverflow, 'mathoverflow'), (df_sx_stackoverflow, 'stackoverflow')]\n",
    "\n",
    "look_around = np.arange(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate temporal subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory(subnetwork):\n",
    "    return 'email' if subnetwork.startswith('dept') else 'sx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds, name in datasets:\n",
    "    # frames will contain the hourly time-frames\n",
    "    print('Processing: {0}\\t[{1}]'.format(name, datetime.now()))\n",
    "    frames = []\n",
    "    # timestamp of the first hour of each time-frame\n",
    "    prev_ts = datetime.utcfromtimestamp(list(ds.head(1)['timestamp'])[0])\n",
    "    prev_ts = datetime(prev_ts.year, prev_ts.month, prev_ts.day, 0, 0, 0)    # CHANGE THIS LATER IF EXECUTING FOR SX NETWORK\n",
    "    # rows for each individual hourly time-frame\n",
    "    rows = []\n",
    "    \n",
    "    for row in ds.itertuples(index=False):\n",
    "        curr_ts = datetime.utcfromtimestamp(row.timestamp)\n",
    "        curr_ts = datetime(curr_ts.year, curr_ts.month, curr_ts.day, 0, 0, 0)\n",
    "\n",
    "        if curr_ts != prev_ts: # went into next time window\n",
    "            frames.append(pd.DataFrame(rows, columns=['from', 'to', 'timestamp']))\n",
    "            # add missing hours from previous timestamp till now as empty dataframes\n",
    "            frames.extend([pd.DataFrame(columns=['from', 'to', 'timestamp'])  for i in range(int((curr_ts-prev_ts).total_seconds() // (24*3600)) - 1)])\n",
    "            rows = []\n",
    "            prev_ts = curr_ts\n",
    "        \n",
    "        # append row to current time-window \n",
    "        rows.append(list(row))\n",
    "    \n",
    "    for la in look_around:\n",
    "        # result_frames will contain all windowed timeframes as explained above\n",
    "        result_frames = []\n",
    "        \n",
    "        # iterate over the hourly frames and construct time-windows frames\n",
    "        for i in range (la, len(frames)-la):\n",
    "            look_around_items = [frames[i+l].values for l in range(-la, la+1)]\n",
    "            result_frames.append(pd.DataFrame(data=np.concatenate(look_around_items, axis=0)))\n",
    "            \n",
    "        directory = get_directory(name)\n",
    "        \n",
    "        # save subgraphs to disk (note: directories should exist before running this cell)\n",
    "        for i, df in enumerate(result_frames):\n",
    "            df.to_csv('data/subgraphs/temporal/{0}/la_{1}/{2}/{3:05d}.csv'.format(directory, la, name, i), sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove empty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dir = 'data/subgraphs/temporal'\n",
    "dataset_subnets = {'email': ['dept_1', 'dept_2', 'dept_3', 'dept_4'], 'sx': ['superuser', 'askubuntu', 'mathoverflow', 'stackoverflow']}\n",
    "look_around_values = ['la_0', 'la_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset, subnets in dataset_subnets.items():\n",
    "    for subnet in subnets:\n",
    "        print('Processing: {0}\\t\\t[{1}]'.format(subnet, datetime.now()))\n",
    "        for la in look_around_values:\n",
    "            delete_from_dir = '{0}/{1}/{2}/{3}'.format(clean_dir, dataset, la, subnet)\n",
    "            \n",
    "            for file in os.listdir(delete_from_dir):\n",
    "                file_location = '{0}/{1}'.format(delete_from_dir, file)\n",
    "                \n",
    "                if os.path.getsize(file_location) == 0:\n",
    "                    os.remove(file_location)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate static subgraphs from the temporal ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/subgraphs/temporal'\n",
    "output_dir = 'data/subgraphs/static'\n",
    "dataset_subnets = {'email': ['dept_1', 'dept_2', 'dept_3', 'dept_4'], 'sx': ['superuser', 'askubuntu', 'mathoverflow', 'stackoverflow']}\n",
    "look_around_values = ['la_0', 'la_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, subnets in dataset_subnets.items():\n",
    "    for subnet in subnets:\n",
    "        print('Processing: {0}\\t[{1}]'.format(subnet, datetime.now()))\n",
    "        for la in look_around_values:\n",
    "            read_from_dir = '{0}/{1}/{2}/{3}'.format(input_dir, dataset, la, subnet)\n",
    "            write_to_dir = '{0}/{1}/{2}/{3}'.format(output_dir, dataset, la, subnet)\n",
    "            \n",
    "            for file in os.listdir(read_from_dir):\n",
    "                input_file_location = '{0}/{1}'.format(read_from_dir, file)\n",
    "                output_file_location = '{0}/{1}'.format(write_to_dir, file)\n",
    "                \n",
    "                # reading the temporal graph file, converting it into static DiGraph and saving it back to disk\n",
    "                df = pd.read_csv(input_file_location, delim_whitespace=True, header=None, names=['from', 'to', 'timestamp'])\n",
    "                G = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())\n",
    "                df = nx.to_pandas_edgelist(G)\n",
    "                df.to_csv(output_file_location, sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_dir = 'data/subgraphs_temporal_motif_distribution'\n",
    "\n",
    "# for dataset, subnets in dataset_subnets.items():\n",
    "#     for subnet in subnets:\n",
    "#         print('Processing: {0}\\t\\t[{1}]'.format(subnet, datetime.now()))\n",
    "#         for la in look_around_values:\n",
    "#             delete_from_dir = '{0}/{1}/{2}/{3}'.format(clean_dir, dataset, la, subnet)\n",
    "            \n",
    "#             for file in os.listdir(delete_from_dir):\n",
    "#                 file_location = '{0}/{1}'.format(delete_from_dir, file)\n",
    "#                 os.remove(file_location)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count temporal motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/subgraphs/temporal'\n",
    "output_dir = 'data/motif_distribution/temporal'\n",
    "dataset_subnets = {'email': ['dept_1', 'dept_2', 'dept_3', 'dept_4'], 'sx': ['superuser', 'askubuntu', 'mathoverflow', 'stackoverflow']}\n",
    "look_around_values = ['la_0', 'la_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, subnets in dataset_subnets.items():\n",
    "    for subnet in subnets:\n",
    "        print('Processing: {0}\\t[{1}]'.format(subnet, datetime.now()))\n",
    "        for la in look_around_values:\n",
    "            read_from_dir = '{0}/{1}/{2}/{3}'.format(input_dir, dataset, la, subnet)\n",
    "            write_to_dir = '{0}/{1}/{2}/{3}'.format(output_dir, dataset, la, subnet)\n",
    "            \n",
    "            for input_file in os.listdir(read_from_dir):\n",
    "                parts = input_file.split(r'.')\n",
    "                output_file = parts[0] + '_motif_istribution.txt'\n",
    "                \n",
    "                input_file_location = '{0}/{1}'.format(read_from_dir, input_file)\n",
    "                output_file_location = '{0}/{1}'.format(write_to_dir, output_file)\n",
    "                \n",
    "                subprocess.run(['temporalmotifsmain', '-i:' + input_file_location, '-delta:3600', '-o:' + output_file_location])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count static motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/subgraphs/static'\n",
    "output_dir = 'data/motif_distribution/static'\n",
    "dataset_subnets = {'email': ['dept_1', 'dept_2', 'dept_3', 'dept_4'], 'sx': ['superuser', 'askubuntu', 'mathoverflow', 'stackoverflow']}\n",
    "look_around_values = ['la_0', 'la_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, subnets in dataset_subnets.items():\n",
    "    for subnet in subnets:\n",
    "        print('Processing: {0}\\t[{1}]'.format(subnet, datetime.now()))\n",
    "        for la in look_around_values:\n",
    "            read_from_dir = '{0}/{1}/{2}/{3}'.format(input_dir, dataset, la, subnet)\n",
    "            write_to_dir = '{0}/{1}/{2}/{3}'.format(output_dir, dataset, la, subnet)\n",
    "            \n",
    "            for input_file in os.listdir(read_from_dir):\n",
    "                parts = input_file.split(r'.')\n",
    "                prefix = parts[0]\n",
    "                \n",
    "                input_file_location = '{0}/{1}'.format(read_from_dir, input_file)\n",
    "                output_file_location = '{0}/{1}'.format(write_to_dir, prefix)\n",
    "                \n",
    "                subprocess.run(['motifs', '-i:' + input_file_location, '-m:3', '-d:F','-o:' + output_file_location])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create csv files from the temporal motif distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/motif_distribution/temporal'\n",
    "output_dir = 'data/csv/temporal'\n",
    "dataset_subnets = {'email': ['dept_1', 'dept_2', 'dept_3', 'dept_4'], 'sx': ['superuser', 'askubuntu', 'mathoverflow', 'stackoverflow']}\n",
    "look_around_values = ['la_0', 'la_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dataset, subnets in dataset_subnets.items():\n",
    "    for subnet in subnets:\n",
    "        print('Processing: {0}\\t[{1}]'.format(subnet, datetime.now()))\n",
    "        for la in look_around_values:\n",
    "            read_from_dir = '{0}/{1}/{2}/{3}'.format(input_dir, dataset, la, subnet)\n",
    "            write_to_dir = '{0}/{1}/{2}'.format(output_dir, dataset, la)\n",
    "            output_file_location = '{0}/{1}.csv'.format(write_to_dir, subnet)\n",
    "            \n",
    "            with open (output_file_location, 'w', newline='') as out_file:\n",
    "                writer = csv.writer(out_file)\n",
    "                \n",
    "                for input_file in os.listdir(read_from_dir):\n",
    "                    input_file_location = '{0}/{1}'.format(read_from_dir, input_file)\n",
    "                    with open (input_file_location, 'r') as in_file:\n",
    "                        values = []\n",
    "                        for line in in_file:\n",
    "                            parts = line.split(\" \")\n",
    "                            values.extend([int(p) for p in parts])\n",
    "                        writer.writerow(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create csv files from the static motif distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/motif_distribution/static'\n",
    "output_dir = 'data/csv/static'\n",
    "dataset_subnets = {'email': ['dept_1', 'dept_2', 'dept_3', 'dept_4'], 'sx': ['superuser', 'askubuntu', 'mathoverflow', 'stackoverflow']}\n",
    "look_around_values = ['la_0', 'la_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset, subnets in dataset_subnets.items():\n",
    "    for subnet in subnets:\n",
    "        print('Processing: {0}\\t[{1}]'.format(subnet, datetime.now()))\n",
    "        for la in look_around_values:\n",
    "            read_from_dir = '{0}/{1}/{2}/{3}'.format(input_dir, dataset, la, subnet)\n",
    "            write_to_dir = '{0}/{1}/{2}'.format(output_dir, dataset, la)\n",
    "            output_file_location = '{0}/{1}.csv'.format(write_to_dir, subnet)\n",
    "            \n",
    "            with open (output_file_location, 'w', newline='') as out_file:\n",
    "                writer = csv.writer(out_file)\n",
    "                \n",
    "                for input_file in os.listdir(read_from_dir):\n",
    "                    input_file_location = '{0}/{1}'.format(read_from_dir, input_file)\n",
    "                    df = pd.read_csv(input_file_location, sep='\\t')\n",
    "                    values = list(df['Count'])\n",
    "                    writer.writerow(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create merged csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/csv'\n",
    "output_dir = 'data/csv/merged'\n",
    "dataset_subnets = {'email': ['dept_1', 'dept_2', 'dept_3', 'dept_4'], 'sx': ['superuser', 'askubuntu', 'mathoverflow', 'stackoverflow']}\n",
    "look_around_values = ['la_0', 'la_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, subnets in dataset_subnets.items():\n",
    "    for subnet in subnets:\n",
    "        print('Processing: {0}\\t[{1}]'.format(subnet, datetime.now()))\n",
    "        for la in look_around_values:\n",
    "            df_static_loc = '{0}/static/{1}/{2}/{3}.csv'.format(input_dir, dataset, la, subnet)\n",
    "            df_temporal_loc = '{0}/temporal/{1}/{2}/{3}.csv'.format(input_dir, dataset, la, subnet)\n",
    "            write_to_loc = '{0}/{1}/{2}/{3}.csv'.format(output_dir, dataset, la, subnet)\n",
    "            \n",
    "            df_static = pd.read_csv(df_static_loc, header=None)\n",
    "            df_static.columns = ['s-'+str(c) for c in df_static.columns] # rename columns so it's the join doesn't have conflicts\n",
    "            \n",
    "            df_temporal = pd.read_csv(df_temporal_loc, header=None)\n",
    "            df_temporal.columns = ['t-'+str(c) for c in df_temporal.columns]\n",
    "            \n",
    "            df = df_static.join(df_temporal)\n",
    "            df.to_csv(write_to_loc, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split\n",
    "We will partition the dataset in a 80-20 split, in such manner that we train on past data and test on future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/csv'\n",
    "output_dir = 'data/datasets'\n",
    "types = ['static', 'temporal', 'merged']\n",
    "datasets = ['email', 'sx']\n",
    "look_around_values = ['la_0', 'la_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in types:\n",
    "    for dataset in datasets:\n",
    "        print('Processing: {0}\\t{1}\\t[{2}]'.format(t, dataset, datetime.now()))\n",
    "        for la in look_around_values:\n",
    "            read_from_dir = '{0}/{1}/{2}/{3}'.format(input_dir, t, dataset, la)\n",
    "            write_to_dir = '{0}/{1}/{2}/{3}'.format(output_dir, t, dataset, la)\n",
    "\n",
    "            output_train_location = '{0}/train.csv'.format(write_to_dir)\n",
    "            output_test_location = '{0}/test.csv'.format(write_to_dir)\n",
    "\n",
    "            train = []\n",
    "            test = []\n",
    "\n",
    "            for input_file in os.listdir(read_from_dir):\n",
    "                input_file_location = '{0}/{1}'.format(read_from_dir, input_file)\n",
    "                parts = input_file.split(r'.')\n",
    "                target_class = parts[0]\n",
    "\n",
    "                df = pd.read_csv(input_file_location, header=None)\n",
    "                df['class'] = target_class\n",
    "                values = df.values\n",
    "                split = int(0.8 * len(values))\n",
    "                train.append(values[:split])\n",
    "                test.append(values[split:])\n",
    "\n",
    "            df_train = pd.DataFrame(data=np.concatenate(train, axis=0))\n",
    "            df_test = pd.DataFrame(data=np.concatenate(test, axis=0))\n",
    "\n",
    "            df_train.to_csv(output_train_location, index=False, header=False)\n",
    "            df_test.to_csv(output_test_location, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
